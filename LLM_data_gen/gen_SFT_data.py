# -*- coding: utf-8 -*-
"""
å¼‚æ­¥æ‰¹é‡ç”Ÿæˆâ€œå“ˆå·¥å¤§â€ç›¸å…³ä¸­æ–‡ SFT æ•°æ®ï¼ˆæŒ‰ ChatML æ¨¡æ¿è½ç›˜ä¸º JSONL çš„ text å­—æ®µï¼‰
- æ”¯æŒä¸¤ç±»æ¨¡å‹ï¼šchatï¼ˆdeepseek-chat ç­‰ï¼‰ä¸ reasonerï¼ˆdeepseek-reasonerï¼‰
- é€šè¿‡ç»Ÿä¸€çš„ LLMCaller å°è£…ï¼Œä¾¿äºåˆ‡æ¢ä¸å¤ç”¨
- å¹¶å‘ã€é‡è¯•ã€å»é‡ã€è´¨é‡æ ¡éªŒã€æ–­ç‚¹ç»­è·‘
"""
"""
âœ… å¦‚ä½•è°ƒç”¨ï¼ˆä¸¤ç§æ¨¡å‹éšæ—¶åˆ‡æ¢ï¼‰
1) ä½¿ç”¨ chatï¼ˆå¦‚ deepseek-chatï¼‰
export OPENAI_API_KEY=ä½ çš„key
export OPENAI_BASE_URL=https://api.deepseek.com

python gen_SFT_data.py \
  --model deepseek-chat \
  --mode chat \
  --out data/raw/hit/train.jsonl \
  --n-per-topic 6 \
  --max-concurrency 50

2) ä½¿ç”¨ reasonerï¼ˆdeepseek-reasonerï¼‰
export OPENAI_API_KEY=ä½ çš„key
export OPENAI_BASE_URL=https://api.deepseek.com

python gen_SFT_data.py \
  --model deepseek-reasoner \
  --mode reasoner \
  --out data/raw/hit/train.jsonl \
  --n-per-topic 6 \
  --max-concurrency 50

deepseek-reasoner çš„ æ€ç»´é“¾ä¼šåœ¨ reasoning_content å­—æ®µè¿”å›ï¼ˆä»£ç é‡Œå·²è®°å½•åˆ° debug æ—¥å¿—ï¼‰ï¼Œ
ä½†è½ç›˜ä»åªç”¨æœ€ç»ˆå›ç­”æ„å»º ChatML æ–‡æœ¬ï¼Œæ»¡è¶³ä½ ä»¬çš„è®­ç»ƒæ ¼å¼ä¸€è‡´æ€§ã€‚
"""

import os
import re
import sys
import json
import time
import hashlib
import random
import logging
import argparse
import asyncio
from typing import List, Dict, Any, Optional

from tqdm import tqdm
from openai import AsyncOpenAI

# =========================================================
# ==== CONFIGï¼ˆå¯é€‰ï¼Œç›´æ¥åœ¨è¿™é‡Œå†™æ­»ï¼›ç•™ç©ºåˆ™ä¸è¦†ç›–ï¼‰====
# =========================================================
# ğŸ”’ å»ºè®®ä»…åœ¨ä¸ªäººç¯å¢ƒä½¿ç”¨ï¼Œé¿å…æŠŠå¯†é’¥æäº¤åˆ°ä»“åº“ã€‚
API_KEY_CODE = "sk-e3ae81b250c74be690f85e02a5a2a7b9"  # ä¾‹å¦‚ "sk-xxxxxxxxxx"; ç•™ç©ºåˆ™ä¸ç”¨ # gen_SFT
BASE_URL_CODE = "https://api.deepseek.com"  # ä¾‹å¦‚ "https://api.deepseek.com"; ç•™ç©ºåˆ™ä¸ç”¨

# å…¶å®ƒè¿è¡Œå‚æ•°ï¼ˆç•™ç©º/Noneåˆ™ä¸è¦†ç›–å‘½ä»¤è¡Œï¼‰
MODEL_CODE = "deepseek-chat"            # ä¾‹å¦‚ "deepseek-chat" æˆ– "deepseek-reasoner"
MODE_CODE = "chat"             # "chat" / "reasoner"
OUT_CODE = "E:/PE_Exam_2025_Autumn/code/gen_SFT_output/generated_data.jsonl"              # ä¾‹å¦‚ "data/raw/hit/train.jsonl"
TOPICS_FILE_CODE =  "E:/PE_Exam_2025_Autumn/code/gen_SFT_output/topics.txt"    # ä¾‹å¦‚ "topics.txt"
N_PER_TOPIC_CODE = 6      # ä¾‹å¦‚ 6
MAX_CONCURRENCY_CODE = 50  # ä¾‹å¦‚ 8
TEMPERATURE_CODE = 0.8      # ä¾‹å¦‚ 0.8
# =========================================================

# =========================
# å¸¸é‡ä¸æ¨¡æ¿
# =========================

DEFAULT_TOPICS = [
    "å“ˆå·¥å¤§è®¡ç®—æœºä¸“ä¸šæŠ¥è€ƒä¸å‡†å¤‡å»ºè®®",
    "å“ˆå·¥å¤§äººå·¥æ™ºèƒ½ä¸“ä¸šè¯¾ç¨‹ä½“ç³»ä¸å­¦ä¹ è·¯å¾„",
    "å“ˆå·¥å¤§æœºæ¢°å·¥ç¨‹/è‡ªåŠ¨åŒ–ä¸“ä¸šé€‰æ‹©å¯¹æ¯”",
    "å“ˆå·¥å¤§å¤§ä¸€é€‰è¯¾ç­–ç•¥ä¸GPAç®¡ç†",
    "å“ˆå·¥å¤§å¸¸è§æŒ‚ç§‘ç§‘ç›®ä¸é¿å…æ–¹æ³•",
    "å“ˆå·¥å¤§äººå·¥æ™ºèƒ½/æœºå™¨äººå®éªŒå®¤ç”³è¯·æµç¨‹ä¸å»ºè®®",
    "æœ¬ç§‘ç”Ÿå‚ä¸ç§‘ç ”ï¼ˆSRTP/å¤§åˆ›/ç§‘åˆ›ï¼‰æŒ‡å—",
    "å“ˆå·¥å¤§æœºå™¨äººç«èµ›/ç”µèµ›/æ•°æ¨¡å¦‚ä½•å‡†å¤‡",
    "ä¿ç ”è·¯å¾„ä¸è€ƒæ ¸è¦ç‚¹",
    "å“ˆå·¥å¤§å­¦ç”Ÿäº’è”ç½‘å®ä¹ /æ ¡æ‹›å‡†å¤‡",
    "ç”³è¯·è‹±ç¾é«˜æ ¡ç¡•å£«/åšå£«çš„å»ºè®®ä¸ææ–™",
    "å—åŒº/ä¸€æ ¡åŒº/äºŒæ ¡åŒºç”Ÿæ´»æŒ‡å—ä¸é£Ÿå ‚æ¨è",
    "å“ˆå°”æ»¨å†¬å­£å¾¡å¯’ä¸å­¦ä¹ æ•ˆç‡å»ºè®®",
    "ç¤¾å›¢ä¸æ—¶é—´ç®¡ç†å¹³è¡¡"
]

CHATML_BEGIN = "<|beginofutterance|>"
CHATML_END = "<|endofutterance|>"
SYSTEM_ROLE = "ç³»ç»Ÿ"
USER_ROLE = "ç”¨æˆ·"
ASSISTANT_ROLE = "æ™ºèƒ½åŠ©æ‰‹"

def build_chatml_text(instruction: str, question: str, answer: str) -> str:
    return (
        f"{CHATML_BEGIN}{SYSTEM_ROLE}\n{instruction}\n{CHATML_END}\n"
        f"{CHATML_BEGIN}{USER_ROLE}\n{question}\n{CHATML_END}\n"
        f"{CHATML_BEGIN}{ASSISTANT_ROLE}\n{answer}\n{CHATML_END}"
    )

GEN_TEMPLATE = """ä½ æ˜¯ä¸€åä¸¥è°¨çš„ä¸­æ–‡æ•™è‚²æ•°æ®æ ‡æ³¨å‘˜ï¼Œè´Ÿè´£ä¸ºâ€œå“ˆå·¥å¤§ç›¸å…³ä¸»é¢˜â€çš„ä¸­æ–‡å¯¹è¯å¾®è°ƒï¼ˆSFTï¼‰ç”Ÿæˆé«˜è´¨é‡æ ·æœ¬ã€‚

ã€è¾“å‡ºè¦æ±‚ã€‘
- **åªè¾“å‡º JSON**ï¼ˆUTF-8ï¼Œæ— æ³¨é‡Šï¼‰ï¼Œæ ¼å¼ä¸ºï¼šä¸€ä¸ªæ•°ç»„ï¼Œæ•°ç»„å†…æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªå¯¹è±¡ï¼Œå­—æ®µä¸ºï¼š
  - instructionï¼šç³»ç»Ÿè§’è‰²å¯¹åŠ©æ‰‹çš„è§’è‰²è®¾å®š/ä»»åŠ¡è¯´æ˜ï¼ˆå¿…é¡»ä¸å“ˆå·¥å¤§ä¸»é¢˜ç›¸å…³ï¼Œä¸­æ–‡æ’°å†™ï¼Œ50~120å­—ï¼‰
  - inputï¼šç”¨æˆ·æé—®ï¼ˆä¸­æ–‡ï¼Œæ˜ç¡®å…·ä½“ï¼Œ30~120å­—ï¼Œå°½é‡ç»“åˆå“ˆå·¥å¤§çœŸå®åœºæ™¯ï¼‰
  - outputï¼šåŠ©æ‰‹å›ç­”ï¼ˆä¸­æ–‡ï¼Œä¸“ä¸šã€å¯ä¿¡ã€ç»“æ„åŒ–ï¼Œ200~500å­—ï¼Œåˆ†ç‚¹å™è¿°ï¼Œé¿å…å¤¸å¼ /è™šå‡ä¿¡æ¯ï¼‰

ã€å†…å®¹è¾¹ç•Œã€‘
- ä¸ç”Ÿæˆè¿æ³•ã€è‰²æƒ…ã€ä»‡æ¨ã€éšç§å†…å®¹
- ä¿æŒä¸­ç«‹ä¸ä¸“ä¸šï¼Œä¸æé€ äº‹å®
- å°½å¯èƒ½ç»“åˆâ€œå“ˆå·¥å¤§â€çœŸå®è¯­å¢ƒï¼ˆè¯¾ç¨‹ã€å®éªŒå®¤ã€ç§‘ç ”ã€ç«èµ›ã€ç”Ÿæ´»ã€åœ°ç‚¹ç­‰ï¼‰

ã€ç”Ÿæˆæ•°é‡ã€‘
- è¯·ä¸ºä¸»é¢˜ã€Š{topic}ã€‹ä¸€æ¬¡æ€§ç”Ÿæˆ {n} æ¡é«˜è´¨é‡æ ·æœ¬
- åªè¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦é¢å¤–æ–‡å­—
"""

def construct_messages(topic: str, n: int) -> List[Dict[str, str]]:
    sys_prompt = "ä½ æ˜¯å®‰å…¨ã€ä¸“ä¸šçš„ä¸­æ–‡æ•™è‚²æ•°æ®æ ‡æ³¨åŠ©æ‰‹ã€‚"
    user_prompt = GEN_TEMPLATE.format(topic=topic, n=n)
    return [
        {"role": "system", "content": sys_prompt},
        {"role": "user", "content": user_prompt}
    ]

# =========================
# .env / ç¯å¢ƒåŠ è½½ / è¦†ç›–å·¥å…·
# =========================

def load_dotenv(path: str = ".env") -> None:
    """æç®€ .env è§£æï¼ˆkey=valueï¼‰ï¼Œåªè®¾ç½®å½“å‰æœªå­˜åœ¨çš„ç¯å¢ƒå˜é‡ã€‚"""
    if not os.path.exists(path):
        return
    try:
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                s = line.strip()
                if not s or s.startswith("#") or "=" not in s:
                    continue
                k, v = s.split("=", 1)
                k, v = k.strip(), v.strip().strip('"').strip("'")
                os.environ.setdefault(k, v)
    except Exception:
        pass

def load_api_key() -> str:
    """
    API Key è·å–ä¼˜å…ˆçº§ï¼š
    1) API_KEY_CODEï¼ˆä»£ç å¸¸é‡ï¼‰
    2) å‘½ä»¤è¡Œï¼ˆåœ¨ amain é‡Œè¦†ç›–ï¼‰
    3) ç¯å¢ƒå˜é‡ OPENAI_API_KEY
    4) .env æ–‡ä»¶ï¼ˆä¼šåœ¨ main å¼€å§‹å°±åŠ è½½ï¼‰
    """
    if API_KEY_CODE:
        return API_KEY_CODE
    # å‘½ä»¤è¡Œåœ¨ parse_args åå¤„ç†
    env_key = os.getenv("OPENAI_API_KEY", "")
    return env_key

def load_base_url() -> str:
    """
    Base URL è·å–ä¼˜å…ˆçº§ï¼š
    1) BASE_URL_CODEï¼ˆä»£ç å¸¸é‡ï¼‰
    2) å‘½ä»¤è¡Œï¼ˆåœ¨ amain é‡Œè¦†ç›–ï¼‰
    3) ç¯å¢ƒå˜é‡ OPENAI_BASE_URL æˆ–é»˜è®¤ https://api.deepseek.com
    """
    if BASE_URL_CODE:
        return BASE_URL_CODE
    return os.getenv("OPENAI_BASE_URL", "https://api.deepseek.com")

def apply_code_overrides(args):
    """ç”¨ä»£ç å¸¸é‡è¦†ç›–å‘½ä»¤è¡Œå‚æ•°ï¼ˆä»…å½“ä»£ç å¸¸é‡éç©º/éNoneæ—¶ï¼‰"""
    if MODEL_CODE is not None:
        args.model = MODEL_CODE
    if MODE_CODE is not None:
        args.mode = MODE_CODE
    if OUT_CODE is not None:
        args.out = OUT_CODE
    if TOPICS_FILE_CODE is not None:
        args.topics_file = TOPICS_FILE_CODE
    if N_PER_TOPIC_CODE is not None:
        args.n_per_topic = N_PER_TOPIC_CODE
    if MAX_CONCURRENCY_CODE is not None:
        args.max_concurrency = MAX_CONCURRENCY_CODE
    if TEMPERATURE_CODE is not None:
        args.temperature = TEMPERATURE_CODE
    return args

# =========================
# è´¨é‡æ ¡éªŒ & å»é‡
# =========================

def is_chinese_ratio_ok(text: str, min_ratio: float = 0.6) -> bool:
    if not text:
        return False
    zh = sum(1 for c in text if '\u4e00' <= c <= '\u9fff')
    return (zh / max(1, len(text))) >= min_ratio

def length_ok(text: str, min_len: int, max_len: int) -> bool:
    L = len(text.strip())
    return min_len <= L <= max_len

def validate_item(obj: Dict[str, Any]) -> Optional[Dict[str, str]]:
    if not isinstance(obj, dict):
        return None
    for k in ("instruction", "input", "output"):
        if k not in obj or not isinstance(obj[k], str):
            return None
    ins, inp, out = obj["instruction"].strip(), obj["input"].strip(), obj["output"].strip()

    if not (is_chinese_ratio_ok(ins) and is_chinese_ratio_ok(inp) and is_chinese_ratio_ok(out)):
        return None
    if not length_ok(ins, 20, 200): return None
    if not length_ok(inp, 20, 300): return None
    if not length_ok(out, 80, 1200): return None

    bad = [r"ä»…ä¾›å‚è€ƒ", r"å…è´£å£°æ˜", r"æŠ±æ­‰æˆ‘ä¸èƒ½", r"æ— æ³•å›ç­”"]
    if any(re.search(p, out) for p in bad):
        return None

    return {"instruction": ins, "input": inp, "output": out}

# =========================
# ç»Ÿä¸€æ¨¡å‹è°ƒç”¨å°è£…ï¼ˆchat / reasonerï¼‰
# =========================

class LLMCaller:
    """
    ç»Ÿä¸€è°ƒç”¨å…¥å£ï¼š
    - mode='chat'     â†’ deepseek-chatï¼ˆæˆ–å…¶å®ƒå…¼å®¹èŠå¤©æ¨¡å‹ï¼‰
    - mode='reasoner' â†’ deepseek-reasonerï¼ˆå¯å– reasoning_content ä½œæ—¥å¿—ï¼‰
    """

    def __init__(self, api_key: str, base_url: str, model: str, mode: str = "chat"):
        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        self.model = model
        assert mode in ("chat", "reasoner")
        self.mode = mode

    async def generate_json_array(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 4096,
        temperature: float = 0.8,
        retries: int = 3,
        backoff: float = 1.8,
        timeout_s: int = 90,
    ) -> List[Dict[str, Any]]:
        last_err = None
        for attempt in range(1, retries + 1):
            try:
                resp = await asyncio.wait_for(
                    self.client.chat.completions.create(
                        model=self.model,
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature if self.mode == "chat" else None,  # reasonerå¿½ç•¥é‡‡æ ·å‚æ•°
                    ),
                    timeout=timeout_s
                )
                if self.mode == "reasoner":
                    try:
                        rc = resp.choices[0].message.reasoning_content
                        if rc:
                            logging.debug("[reasoner reasoning_content]\n" + rc[:2000])
                    except Exception:
                        pass

                content = resp.choices[0].message.content
                m = re.search(r"(\[.*\])", content, flags=re.S)
                if not m:
                    raise ValueError("æœªåœ¨æ¨¡å‹è¾“å‡ºä¸­å®šä½åˆ° JSON æ•°ç»„")
                data = json.loads(m.group(1))
                if not isinstance(data, list):
                    raise ValueError("æå–åˆ°çš„ JSON ä¸æ˜¯æ•°ç»„")
                return data
            except Exception as e:
                last_err = e
                sleep_s = (backoff ** (attempt - 1)) * 0.8 + random.random() * 0.2
                await asyncio.sleep(sleep_s)
        raise RuntimeError(f"LLM è°ƒç”¨å¤±è´¥ï¼ˆå·²é‡è¯• {retries} æ¬¡ï¼‰: {last_err}")

# =========================
# ä¸»æµç¨‹ï¼šå¹¶å‘ç”Ÿæˆ + ChatML è½ç›˜
# =========================

async def generate_for_topics(
    topics: List[str],
    n_per_topic: int,
    caller: LLMCaller,
    out_jsonl: str,
    max_concurrency: int = 6,
    temperature: float = 0.8,
) -> None:
    os.makedirs(os.path.dirname(out_jsonl) or ".", exist_ok=True)

    # æ–­ç‚¹ç»­è·‘ï¼šè¯»å–å·²å­˜åœ¨è®°å½•
    seen = set()
    if os.path.exists(out_jsonl):
        with open(out_jsonl, "r", encoding="utf-8") as f:
            for ln in f:
                try:
                    obj = json.loads(ln)
                    text = obj.get("text", "")
                    fp = hashlib.md5(text.encode("utf-8")).hexdigest()
                    seen.add(fp)
                except Exception:
                    continue

    lock = asyncio.Lock()
    sem = asyncio.Semaphore(max_concurrency)

    async def worker(topic: str):
        async with sem:
            msgs = construct_messages(topic, n_per_topic)
            data = await caller.generate_json_array(
                messages=msgs,
                temperature=temperature,
                max_tokens=4096,
            )

            out_records = []
            for raw in data:
                item = validate_item(raw)
                if not item:
                    continue
                chatml = build_chatml_text(item["instruction"], item["input"], item["output"])
                fp = hashlib.md5(chatml.encode("utf-8")).hexdigest()
                if fp in seen:
                    continue
                out_records.append({"text": chatml})

            if out_records:
                async with lock:
                    with open(out_jsonl, "a", encoding="utf-8") as f:
                        for obj in out_records:
                            f.write(json.dumps(obj, ensure_ascii=False) + "\n")
                            seen.add(hashlib.md5(obj["text"].encode("utf-8")).hexdigest())

    tasks = [asyncio.create_task(worker(t)) for t in topics]
    for fut in tqdm(asyncio.as_completed(tasks), total=len(tasks), desc="Generating"):
        try:
            await fut
        except Exception as e:
            logging.error(f"[Topic å¤±è´¥] {e}")

# =========================
# CLI
# =========================

def read_lines(path: str) -> List[str]:
    with open(path, "r", encoding="utf-8") as f:
        return [ln.strip() for ln in f if ln.strip()]

def setup_logging(level=logging.INFO):
    logging.basicConfig(
        level=level,
        format="%(asctime)s | %(levelname)s | %(message)s",
        handlers=[logging.StreamHandler(sys.stdout)],
    )

def parse_args():
    ap = argparse.ArgumentParser(description="å¼‚æ­¥ç”Ÿæˆå“ˆå·¥å¤§ç›¸å…³ SFT æ•°æ®ï¼ˆJSONLï¼Œtext=ChatML æ¨¡æ¿ï¼‰")
    ap.add_argument("--api-key", type=str, default="", help="ä¼˜å…ˆçº§ä½äºä»£ç å¸¸é‡ï¼Œæ™šäº .env å’Œç¯å¢ƒå˜é‡")
    ap.add_argument("--base-url", type=str, default="", help="ä¼˜å…ˆçº§ä½äºä»£ç å¸¸é‡ï¼Œæ™šäº .env å’Œç¯å¢ƒå˜é‡")
    ap.add_argument("--model", type=str, default="deepseek-chat",
                    help="å¯ç”¨ deepseek-chat æˆ– deepseek-reasoner")
    ap.add_argument("--mode", type=str, default="chat", choices=["chat", "reasoner"],
                    help="chat / reasonerï¼ˆreasoner å°†è®°å½• reasoning_contentï¼‰")
    ap.add_argument("--out", type=str, default="data/raw/hit/train.jsonl")
    ap.add_argument("--topics-file", type=str, default="")
    ap.add_argument("--n-per-topic", type=int, default=6)
    ap.add_argument("--max-concurrency", type=int, default=6)
    ap.add_argument("--temperature", type=float, default=0.8,
                    help="å¯¹ reasoner ä¸ç”Ÿæ•ˆï¼Œä½†ä¿ç•™å‚æ•°å…¼å®¹")
    return ap.parse_args()

async def amain():
    setup_logging()

    # 1) å…ˆåŠ è½½ .envï¼ˆè‹¥å­˜åœ¨ï¼‰
    load_dotenv(".env")

    # 2) è§£æå‘½ä»¤è¡Œ
    args = parse_args()

    # 3) åº”ç”¨ä»£ç å¸¸é‡è¦†ç›–ï¼ˆä½ åœ¨æ–‡ä»¶å¤´éƒ¨å†™çš„ *_CODEï¼‰
    args = apply_code_overrides(args)

    # 4) API Key / Base URL å–å€¼ï¼ˆè€ƒè™‘ä»£ç å¸¸é‡ã€å‘½ä»¤è¡Œã€ç¯å¢ƒå˜é‡ã€.envï¼‰
    api_key = API_KEY_CODE or (args.api_key if args.api_key else load_api_key())
    base_url = BASE_URL_CODE or (args.base_url if args.base_url else load_base_url())

    if not api_key:
        print("è¯·æä¾› API Keyï¼šå¯åœ¨æ–‡ä»¶å¤´ API_KEY_CODE å†™æ­»ï¼Œæˆ– --api-key ä¼ å…¥ï¼Œæˆ–è®¾ç½® OPENAI_API_KEY / .env", file=sys.stderr)
        sys.exit(2)

    topics = DEFAULT_TOPICS
    if args.topics_file:
        topics = read_lines(args.topics_file)
        if not topics:
            print(f"topics æ–‡ä»¶ä¸ºç©ºï¼š{args.topics_file}", file=sys.stderr)
            sys.exit(3)

    caller = LLMCaller(
        api_key=api_key,
        base_url=base_url,
        model=args.model,
        mode=args.mode,
    )

    logging.info(f"å‡†å¤‡ç”Ÿæˆï¼š{len(topics)} ä¸ªä¸»é¢˜ï¼Œæ¯ä¸ª {args.n_per_topic} æ¡ â†’ {args.out}")
    await generate_for_topics(
        topics=topics,
        n_per_topic=args.n_per_topic,
        caller=caller,
        out_jsonl=args.out,
        max_concurrency=args.max_concurrency,
        temperature=args.temperature,
    )
    logging.info("ç”Ÿæˆå®Œæˆ")

if __name__ == "__main__":
    try:
        asyncio.run(amain())
    except KeyboardInterrupt:
        print("ç”¨æˆ·ä¸­æ–­ã€‚")
